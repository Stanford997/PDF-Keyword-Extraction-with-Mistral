{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T15:48:45.867819Z",
     "start_time": "2024-04-28T15:48:33.183018Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade git+https://github.com/UKPLab/sentence-transformers\n",
    "!pip install keybert ctransformers[cuda]\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers\n",
    "!pip install PyPDF2\n",
    "!pip install pandas\n",
    "!pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "with open('Session 13- Transformers and Attention.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    data = {'Page Number': [], 'Text': []}\n",
    "    all_text = \"\"\n",
    "\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            data['Page Number'].append(i + 1)\n",
    "            data['Text'].append(text)\n",
    "            all_text += text + \" \"\n",
    "        else:\n",
    "            data['Page Number'].append(i + 1)\n",
    "            data['Text'].append(\"\")\n",
    "\n",
    "df = pd.DataFrame(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f3595e1d15399f1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 25\u001B[0m\n\u001B[1;32m     23\u001B[0m grouped_texts_list \u001B[38;5;241m=\u001B[39m group_text_by_pages(df, \u001B[38;5;241m20\u001B[39m)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m grouped_texts_list:\n\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28mprint\u001B[39m(text)\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def group_text_by_pages(df, pages_per_group=1):\n",
    "    grouped_texts = []\n",
    "    total_pages = len(df)\n",
    "\n",
    "    for start in range(0, total_pages, pages_per_group):\n",
    "        end = start + pages_per_group\n",
    "        grouped_text = \" \".join(df['Text'][start:end])\n",
    "\n",
    "        grouped_text = re.sub(r'[^\\x00-\\x7F]+', ' ', grouped_text)\n",
    "        grouped_text = re.sub(r\"[\\\"'‘’]\", '', grouped_text)\n",
    "        grouped_text = re.sub(r'\\s+', ' ', grouped_text).strip()\n",
    "\n",
    "        grouped_texts.append(grouped_text)\n",
    "\n",
    "    return grouped_texts\n",
    "\n",
    "\n",
    "grouped_texts_list = group_text_by_pages(df, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T15:54:09.422853Z",
     "start_time": "2024-04-28T15:53:36.207403Z"
    }
   },
   "id": "d04ea007f57e5076"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ctransformers'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mctransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForCausalLM,AutoConfig\n\u001B[1;32m      3\u001B[0m config \u001B[38;5;241m=\u001B[39m AutoConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTheBloke/Mistral-7B-v0.1-GGUF\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Explicitly set the max_seq_len\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'ctransformers'"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM,AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"TheBloke/Mistral-7B-v0.1-GGUF\")\n",
    "# Explicitly set the max_seq_len\n",
    "config.config.max_new_tokens = 2048\n",
    "config.config.context_length = 4096\n",
    "\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "    model_type=\"mistral\",\n",
    "    device_map = 'cuda',\n",
    "    hf=True,\n",
    "    config=config\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T15:57:23.081562Z",
     "start_time": "2024-04-28T15:57:23.023932Z"
    }
   },
   "id": "a6d1eda9dd88afe5"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 11:58:12.695916: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/caozhen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50933cd948f54902ab3e8fef1de529da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa22ce01352a4159b0f27ba7581c32e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe045f719cf8472ba4f16253a8dd4087"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20a2bcc0cf034b8fa000f3c7a70d33cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 11\u001B[0m\n\u001B[1;32m      7\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Pipeline\u001B[39;00m\n\u001B[1;32m     10\u001B[0m generator \u001B[38;5;241m=\u001B[39m pipeline(\n\u001B[0;32m---> 11\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[43mmodel\u001B[49m, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[1;32m     12\u001B[0m     task\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     13\u001B[0m     max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m,\n\u001B[1;32m     14\u001B[0m     repetition_penalty\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.1\u001B[39m\n\u001B[1;32m     15\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "# Pipeline\n",
    "generator = pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    max_new_tokens=50,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T15:58:42.151824Z",
     "start_time": "2024-04-28T15:57:33.757292Z"
    }
   },
   "id": "32f5a4bfa8c8d6ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_prompt = \"\"\"\n",
    "<s>[INST]\n",
    "I have the following document:\n",
    "- The website mentions that it only takes a couple of days to deliver but I still have not received mine.\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else. For example, don't say:\n",
    "\"Here are the keywords present in the document\"\n",
    "[/INST] meat, beef, eat, eating, emissions, steak, food, health, processed, chicken</s>\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-28T15:58:42.147416Z"
    }
   },
   "id": "1840510e05d42ae2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keyword_prompt = \"\"\"\n",
    "[INST]\n",
    "\n",
    "I have the following document:\n",
    "- [DOCUMENT]\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else. For example, don't say:\n",
    "\"Here are the keywords present in the document\"\n",
    "[/INST]\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-28T15:58:42.150996Z"
    }
   },
   "id": "1ca9790a6c1b9b75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = example_prompt + keyword_prompt\n",
    "print(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-28T15:58:42.154289Z"
    }
   },
   "id": "7ea9fe44cfdc1195"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keybert.llm import TextGeneration\n",
    "from keybert import KeyLLM\n",
    "\n",
    "# Load it in KeyLLM\n",
    "llm = TextGeneration(generator, prompt=prompt)\n",
    "kw_model = KeyLLM(llm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-28T15:58:42.157155Z"
    }
   },
   "id": "9e102b0073dbdba4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keywords = kw_model.extract_keywords(grouped_texts_list); \n",
    "keywords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f79e9d6fff9ce1c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
